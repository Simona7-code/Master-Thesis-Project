{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34a4b7e-d1a6-4094-8696-582623baf0e6",
   "metadata": {},
   "source": [
    "# Preparazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba46b89-a4b2-41f2-bca1-e0d75f2ca474",
   "metadata": {},
   "source": [
    "## Preparazione dei dati\n",
    "Processo di generazione dei fold illustrato per le 3 tipologie di dataset creati per le 2 lingue:\n",
    "\n",
    "- Tipo Dataset 1: composto dalla giustapposizione dei set di caratteristiche linguistiche dei 2 saggi;\n",
    "- Tipo Dataset 2: composto dalla differenza di valori per ogni caratteristica linguistica dei set dei 2 saggi;\n",
    "- Tipo Dataset 3: composto dalla differenza di valori per ogni caratteristica linguistica e dalla giustapposizione dei set dei 2 saggi.\n",
    "\n",
    "### Generazione folds - Tipo Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9cbb1-81af-45bd-b479-e07f80d5e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/kfold_DoubleFeat'\n",
    "# Lettura del dataframe contenente gli ID dei saggi e le feature legate all'ordine\n",
    "dtype = {'Essay_1': str, 'Essay_2': str}\n",
    "df= pd.read_csv('Training_CItA.tsv', delimiter=\"\\t\",  dtype=dtype)  \n",
    "# Lettura del DataFrame contenente gli ID dei saggi con annesse feature linguistiche estratte tramite ProfilingUD\n",
    "df_5497_2 = pd.read_csv('profilingUD/5497.csv', sep='\\t')\n",
    "\n",
    "# Numero di folds\n",
    "n_splits = 10\n",
    "\n",
    "# Se non esiste l'output directory la crea\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Inizializzazione dell'oggetto per realizzare la k-fold cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Itera sui fold e divide i dati nei set selezionandone gli indici\n",
    "for i, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "    # Crea cartela per il fold corrente\n",
    "    fold_dir = os.path.join(output_dir, f'fold_{i}')\n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.makedirs(fold_dir)\n",
    "\n",
    "    # Seleziona le righe che corrispondono agli indici per il training e test set\n",
    "    train_data = df.iloc[train_index]\n",
    "    test_data = df.iloc[test_index]\n",
    "    \n",
    "    # Creo una copia dei dataset di training e test del fold corrente\n",
    "    df_2_train = train_data.copy()\n",
    "    df_2_test = test_data.copy()\n",
    "    \n",
    "    # Effettuo uno switch dell'ordine dei set di feature \n",
    "    # Processo per il training set\n",
    "    \n",
    "    # Crea una copia di Essay_1 in una variabile temporanea\n",
    "    temp = df_2_train['Essay_1'].copy() \n",
    "    # Assegna i valori di Essay_2 ad Essay_1\n",
    "    df_2_train['Essay_1'] = df_2_train['Essay_2'] \n",
    "    # Assegna i valori temporanei ad Essay_2\n",
    "    df_2_train['Essay_2'] = temp \n",
    "    \n",
    "    # Crea una copia di Order_1 in una variabile temporanea\n",
    "    temp2 = df_2_train['Order_1'].copy() \n",
    "    # Assegna i valori di Order_2 ad Order_1\n",
    "    df_2_train['Order_1'] = df_2_train['Order_2'] \n",
    "    # Assegna i valori temporanei ad Order_2\n",
    "    df_2_train['Order_2'] = temp2 \n",
    "    #(Processo ripetuto per il test set)\n",
    "    \n",
    "    # Ciclo per il training set:\n",
    "    # Itera sul DataFrame di training contenente gli ID dei saggi e le feature legate all'ordine\n",
    "    for index, row in df_2_train.iterrows():\n",
    "\n",
    "        # Costruzione della chiave di merge per l'ID del saggio Essay_1\n",
    "        key_of_merge = \"text_CIItA/\" + str(row['Essay_1']) + \".conllu\"\n",
    "\n",
    "        # Cerca la chiave di merge nel dataset contenente le features\n",
    "        match_row = df_5497_2[df_5497_2['Filename'] == key_of_merge]\n",
    "\n",
    "        # Se esiste una corrispondenza\n",
    "        if not match_row.empty:\n",
    "            \n",
    "            # Rinomina le colonne di feature contenute in match_row aggiungendo l'indicazione del saggio di appartenenza Essay_1\n",
    "            match_row = match_row.rename(columns={col: col + '_Essay1' for col in match_row.columns})\n",
    "           \n",
    "            # Unisce queste colonne al dataframe contenente gli ID dei saggi e le feature legate all'ordine\n",
    "            for column in match_row.columns:\n",
    "                df_2_train.loc[index, column] = match_row.loc[match_row.index[0], column]\n",
    "                \n",
    "        # Costruzione della chiave di merge per l'ID del saggio Essay_2\n",
    "        key_of_merge_2 = \"text_CIItA/\" + str(row['Essay_2']) + \".conllu\"\n",
    "\n",
    "        # Cerca la chiave di merge nel dataset contenente le features\n",
    "        match_row_2 = df_5497_2[df_5497_2['Filename'] == key_of_merge_2]\n",
    "\n",
    "        # Se esiste una corrispondenza\n",
    "        if not match_row_2.empty:\n",
    "            \n",
    "            # Rinomina le colonne di feature contenute in match_row aggiungendo l'indicazione del saggio di appartenenza Essay_2\n",
    "            match_row_2 = match_row_2.rename(columns={col: col + '_Essay2' for col in match_row_2.columns})\n",
    "            \n",
    "            # Unisce queste colonne al dataframe contenente gli ID dei saggi e le feature legate all'ordine\n",
    "            for column in match_row_2.columns:\n",
    "                 df_2_train.loc[index, column] = match_row_2.loc[match_row_2.index[0], column]\n",
    "                 \n",
    "    # Ciclo per il test set:\n",
    "    # Assegnazione label 0 agli elementi nel dataframe di training invertito \n",
    "    df_2_train['Order_Label'] = 0\n",
    "    # Concatenazione dei dataframe di training con label 0 e 1 per il fold corrente\n",
    "    df_train_final = pd.concat([train_data, df_2_train], axis=0, ignore_index=True)   \n",
    "    \n",
    "    # Assegnazione ripetuta per test:\n",
    "    # Salvataggio dei dataframe di training e test del fold corrente\n",
    "    df_train_final.to_csv(os.path.join(fold_dir, f'train.tsv'), sep='\\t', index=False)\n",
    "    df_test_final.to_csv(os.path.join(fold_dir, f'test.tsv'), sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8631e9-c4f4-4ae5-913d-e9a80aa417b7",
   "metadata": {},
   "source": [
    "### Generazione folds - Tipo Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc91ec-04a5-44bb-8706-14b3e1c1cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisco numero di fold\n",
    "num_folds =10\n",
    "# Ciclo sui fold (processo ripetuto anche per il test set)\n",
    "for i in range(num_folds):\n",
    "    # Lettura dei dati di addestramento dai fold dove sono contenuti i set di feature di entrambi i saggi\n",
    "    train_data = pd.read_csv(f\"kfold_DoubleFeat/fold_{i}/train.tsv\", sep='\\t')\n",
    "\n",
    "    # Otteniamo il numero totale di righe del DataFrame\n",
    "    num_rows = train_data.shape[0]\n",
    "    # Selezioniamo la metà delle righe\n",
    "    half_rows = num_rows // 2\n",
    "\n",
    "    # DataFrame dalla prima metà del set (ordine cronologico)\n",
    "    df1_cronologico = train_data.iloc[:half_rows]  \n",
    "    # DataFrame dalla seconda metà del set (ordine invertito)\n",
    "    df2_NOTCronologico = train_data.iloc[half_rows:]  \n",
    "\n",
    "    # Resetto l'indice sul set di dati con ordine invertito\n",
    "    df2_NOTCronologico= df2_NOTCronologico.reset_index(drop=True)\n",
    "    # Elimino colonne dove sono contenute le feature dei rispettivi Essay_2 da entrambi i set\n",
    "    df1_cronologico = df1_cronologico.drop(columns=df1_cronologico.columns[df1_cronologico.columns.get_loc ('n_sentences_Essay2'):])\n",
    "    df2_NOTCronologico = df2_NOTCronologico.drop(columns=df2_NOTCronologico.columns[df2_NOTCronologico.columns.get_loc ('n_sentences_Essay2'):])\n",
    "    \n",
    "    # Sottraggo blocco di feature di Essay_1 da Essay_2 e appongo la label 1 (ordine cronologico)\n",
    "    new_train_data_1= df1_cronologico.sub(df2_NOTCronologico)\n",
    "    new_train_data_1['Order_Label'] = 1\n",
    "    \n",
    "    # Sottraggo blocco di feature di Essay_2 da Essay_1 e appongo la label 0 (ordine invertito)\n",
    "    new_train_data_2= df2_NOTCronologico.sub(df1_cronologico)\n",
    "    new_train_data_2['Order_Label'] = 0\n",
    "    \n",
    "    # Concateno le differenze di set in un unico dataframe\n",
    "    new_train_data = pd.concat([new_train_data_1, new_train_data_2], axis=0, ignore_index=True) \n",
    "      \n",
    "    # Crea la directory se non esiste\n",
    "    if not os.path.exists(f\"kfold_FeaturesDifference/fold_{i}\"):\n",
    "        os.makedirs(f\"kfold_FeaturesDifference/fold_{i}\")\n",
    "        \n",
    "    # Salva il nuovo DataFrame in un file CSV nella directory corretta\n",
    "    new_train_data.to_csv(f\"kfold_FeaturesDifference/fold_{i}/train.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc0634-e0a7-46ae-a125-3ea9d42c8c0c",
   "metadata": {},
   "source": [
    "### Generazione folds - Tipo Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d9094-362a-4056-a165-728e735e7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di fold\n",
    "num_folds = 10\n",
    "\n",
    "# Cartella in cui salvare i dataframe\n",
    "output_folder = 'kfold_Combo_and_Difference'\n",
    "# Crea la cartella di output\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Ciclo sui fold\n",
    "for i in range(num_folds):\n",
    "   \n",
    "    # Percorso delle cartelle kfold_FeaturesDifference e kfold_DoubleFeat\n",
    "    folder_path_1 = f'kfold_FeaturesDifference/fold_{i}'\n",
    "    folder_path_2 = f'kfold_DoubleFeat/fold_{i}'\n",
    "    \n",
    "    # Crea cartella di output per il fold corrente\n",
    "    output_fold_folder = os.path.join(output_folder, f\"fold_{i}\")\n",
    "    os.makedirs(output_fold_folder, exist_ok=True)\n",
    "    \n",
    "    # Carica i dataframe di test e training dal fold corrente di kfold_DoubleFeat\n",
    "    df_1_test = pd.read_csv(os.path.join(folder_path_1, 'test.tsv'), sep='\\t')\n",
    "    df_1_train = pd.read_csv(os.path.join(folder_path_1, 'train.tsv'), sep='\\t')\n",
    "     # Carica i dataframe di test e training dal fold corrente di kfold_FeaturesDifference\n",
    "    df_2_test = pd.read_csv(os.path.join(folder_path_2, 'test.tsv'), sep='\\t')\n",
    "    df_2_train = pd.read_csv(os.path.join(folder_path_2, 'train.tsv'), sep='\\t')\n",
    "    \n",
    "    # Se i valori di Order_Label dei dataframe di training e test corrispondono droppo una delle due colonne di label\n",
    "    if (df_1_test['Order_Label'] == df_2_test['Order_Label']).all():\n",
    "        df_2_test = df_2_test.drop('Order_Label', axis=1)\n",
    "    if (df_1_train['Order_Label'] == df_2_train['Order_Label']).all():\n",
    "        df_2_train = df_2_train.drop('Order_Label', axis=1)\n",
    "    \n",
    "    # Concateno i dataframe di test e training \n",
    "    merged_test = pd.concat([df_2_test, df_1_test], axis=1)\n",
    "    merged_train = pd.concat([df_2_train, df_1_train], axis=1)\n",
    "    \n",
    "    # Salva i dataframe combinati in nuovi file CSV nella cartella di output\n",
    "    merged_test.to_csv(os.path.join(output_fold_folder, \"test.tsv\"), index=False, sep='\\t')\n",
    "    merged_train.to_csv(os.path.join(output_fold_folder, \"train.tsv\"), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf4b78-0d8a-4d79-bf00-c20f09f49bb5",
   "metadata": {},
   "source": [
    "## Creazione dei dataset di training, test, validation per l'addestramento di Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba347b77-5375-4b3c-a0d3-0dfe2562e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende in input un dataframe e ne restituisce uno arricchito con i record di ordine invertito\n",
    "def modify_dataframe(df):\n",
    "    \n",
    "    # Copia il dataFrame \n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Inverte le colonne contenenti gli ID dei saggi e dei testi \n",
    "    df_copy[['Essay_1', 'Essay_2']] = df_copy[['Essay_2', 'Essay_1']]\n",
    "    df_copy[['Essay_1_Text', 'Essay_2_Text']] = df_copy[['Essay_2_Text', 'Essay_1_Text']]\n",
    "    # Crea nuova colonna contenente i due saggi separati dal token [SEP]\n",
    "    df_copy['Formatted_Text'] = df_copy['Essay_1_Text'] + ' [SEP] ' + df_copy['Essay_2_Text']\n",
    "\n",
    "    # Assegna il valore 0 agli elementi in Order_Label\n",
    "    df_copy['Order_Label'] = 0\n",
    "\n",
    "    # Concatena il DataFrame originale e il DataFrame modificato e lo restituisce\n",
    "    df_final = pd.concat([df, df_copy], axis=0, ignore_index=True)\n",
    "    return df_final\n",
    "\n",
    "# Lettura del dataframe contenente gli ID e i saggi\n",
    "dtype = {'Essay_1': str, 'Essay_2': str}\n",
    "df= pd.read_csv('Training_CItA.csv', delimiter=\",\",  dtype=dtype)  \n",
    "\n",
    "# Crea una copia del dataframe\n",
    "train_150 = df.copy()\n",
    "\n",
    "# Effettua separazione grezza delle stringhe in base agli spazi\n",
    "train_150['Essay_1_Text'] = train_150['Essay_1_Text'].str.split()\n",
    "train_150['Essay_2_Text'] = train_150['Essay_2_Text'].str.split()\n",
    "\n",
    "# Seleziona solo le prime 150 unità per ogni saggio\n",
    "train_150['Essay_1_Text'] = train_150['Essay_1_Text'].apply(lambda x: ' '.join(x[:150]))\n",
    "train_150['Essay_2_Text'] = train_150['Essay_2_Text'].apply(lambda x: ' '.join(x[:150]))\n",
    "\n",
    "# Crea la colonna 'Formatted_Text' contenente i saggi troncati e separati dal token [SEP]\n",
    "train_150['Formatted_Text'] = train_150['Essay_1_Text'] + ' [SEP] ' + train_150['Essay_2_Text']\n",
    "\n",
    "# Creo set di test e validation\n",
    "train_df_150, temp_df_150 = train_test_split(train_150, test_size=0.3, random_state=42)\n",
    "test_df_150, val_df_150 = train_test_split(temp_df_150, test_size=0.1, random_state=42)\n",
    "\n",
    "# Chiama la funzione per ottenere i dataframe con i record di ordine invertito\n",
    "train_with0_150 = modify_dataframe(train_df_150)\n",
    "test_with0_150 = modify_dataframe(test_df_150)\n",
    "val_with0_150 = modify_dataframe(val_df_150)\n",
    "\n",
    "# Salva i dataframe modificato in file CSV\n",
    "train_with0_150.to_csv('Train_CItA_Finetuning.csv', index=False)\n",
    "test_with0_150.to_csv('Test_CItA_Finetuning.csv', index=False)\n",
    "val_with0_150.to_csv('Val_CItA_Finetuning.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
